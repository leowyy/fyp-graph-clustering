{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Transfer Learning Tutorial\n",
    "==========================\n",
    "**Author**: `Sasank Chilamkurthy <https://chsasank.github.io>`_\n",
    "\n",
    "In this tutorial, you will learn how to train your network using\n",
    "transfer learning. You can read more about the transfer learning at `cs231n\n",
    "notes <http://cs231n.github.io/transfer-learning/>`__\n",
    "\n",
    "Quoting these notes,\n",
    "\n",
    "    In practice, very few people train an entire Convolutional Network\n",
    "    from scratch (with random initialization), because it is relatively\n",
    "    rare to have a dataset of sufficient size. Instead, it is common to\n",
    "    pretrain a ConvNet on a very large dataset (e.g. ImageNet, which\n",
    "    contains 1.2 million images with 1000 categories), and then use the\n",
    "    ConvNet either as an initialization or a fixed feature extractor for\n",
    "    the task of interest.\n",
    "\n",
    "These two major transfer learning scenarios look as follows:\n",
    "\n",
    "-  **Finetuning the convnet**: Instead of random initializaion, we\n",
    "   initialize the network with a pretrained network, like the one that is\n",
    "   trained on imagenet 1000 dataset. Rest of the training looks as\n",
    "   usual.\n",
    "-  **ConvNet as fixed feature extractor**: Here, we will freeze the weights\n",
    "   for all of the network except that of the final fully connected\n",
    "   layer. This last fully connected layer is replaced with a new one\n",
    "   with random weights and only this layer is trained.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# License: BSD\n",
    "# Author: Sasank Chilamkurthy\n",
    "\n",
    "from __future__ import print_function, division\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import torchvision\n",
    "from torchvision import datasets, models, transforms\n",
    "import matplotlib.pyplot as plt\n",
    "import time\n",
    "import os\n",
    "import copy\n",
    "\n",
    "plt.ion()   # interactive mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Load Data\n",
    "---------\n",
    "\n",
    "We will use torchvision and torch.utils.data packages for loading the\n",
    "data.\n",
    "\n",
    "The problem we're going to solve today is to train a model to classify\n",
    "**ants** and **bees**. We have about 120 training images each for ants and bees.\n",
    "There are 75 validation images for each class. Usually, this is a very\n",
    "small dataset to generalize upon, if trained from scratch. Since we\n",
    "are using transfer learning, we should be able to generalize reasonably\n",
    "well.\n",
    "\n",
    "This dataset is a very small subset of imagenet.\n",
    "\n",
    ".. Note ::\n",
    "   Download the data from\n",
    "   `here <https://download.pytorch.org/tutorial/hymenoptera_data.zip>`_\n",
    "   and extract it to the current directory.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "os.chdir('..')\n",
    "os.getcwd()\n",
    "from util.cifar_data_loader import get_train_val_set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Files already downloaded and verified\n",
      "Files already downloaded and verified\n"
     ]
    }
   ],
   "source": [
    "parent_dir = os.path.abspath('..')\n",
    "data_dir = parent_dir + '/data/'\n",
    "train_data, val_data, train_sampler, val_sampler = get_train_val_set(data_dir=data_dir, augment=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_datasets = {'train': train_data, 'val': val_data}\n",
    "image_samplers = {'train': train_sampler, 'val': val_sampler}\n",
    "dataloaders = {x: torch.utils.data.DataLoader(image_datasets[x], batch_size=4,\n",
    "                                             sampler=image_samplers[x], num_workers=2, pin_memory=False)\n",
    "              for x in ['train', 'val']}\n",
    "dataset_sizes = {x: len(image_datasets[x]) for x in ['train', 'val']}\n",
    "class_names = ['plane', 'car', 'bird', 'cat', 'deer', 'dog', 'frog', 'horse', 'ship', 'truck']\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualize a few images\n",
    "^^^^^^^^^^^^^^^^^^^^^^\n",
    "Let's visualize a few training images so as to understand the data\n",
    "augmentations.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def imshow(inp, title=None):\n",
    "    \"\"\"Imshow for Tensor.\"\"\"\n",
    "    inp = inp.numpy().transpose((1, 2, 0))\n",
    "    mean = np.array([0.485, 0.456, 0.406])\n",
    "    std = np.array([0.229, 0.224, 0.225])\n",
    "    inp = std * inp + mean\n",
    "    inp = np.clip(inp, 0, 1)\n",
    "    plt.imshow(inp)\n",
    "    if title is not None:\n",
    "        plt.title(title)\n",
    "    plt.pause(0.001)  # pause a bit so that plots are updated\n",
    "\n",
    "\n",
    "# Get a batch of training data\n",
    "inputs, classes = next(iter(dataloaders['train']))\n",
    "\n",
    "# Make a grid from batch\n",
    "out = torchvision.utils.make_grid(inputs)\n",
    "\n",
    "imshow(out, title=[class_names[x] for x in classes])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Training the model\n",
    "------------------\n",
    "\n",
    "Now, let's write a general function to train a model. Here, we will\n",
    "illustrate:\n",
    "\n",
    "-  Scheduling the learning rate\n",
    "-  Saving the best model\n",
    "\n",
    "In the following, parameter ``scheduler`` is an LR scheduler object from\n",
    "``torch.optim.lr_scheduler``.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, criterion, optimizer, scheduler, num_epochs=25):\n",
    "    since = time.time()\n",
    "\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_acc = 0.0\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                scheduler.step()\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()   # Set model to evaluate mode\n",
    "\n",
    "            running_loss = 0.0\n",
    "            running_corrects = 0\n",
    "\n",
    "            # Iterate over data.\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = inputs.to(device)\n",
    "                labels = labels.to(device)\n",
    "\n",
    "                # zero the parameter gradients\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    outputs = model(inputs)\n",
    "                    _, preds = torch.max(outputs, 1)\n",
    "                    loss = criterion(outputs, labels)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                running_loss += loss.item() * inputs.size(0)\n",
    "                running_corrects += torch.sum(preds == labels.data)\n",
    "\n",
    "            epoch_loss = running_loss / dataset_sizes[phase]\n",
    "            epoch_acc = running_corrects.double() / dataset_sizes[phase]\n",
    "\n",
    "            print('{} Loss: {:.4f} Acc: {:.4f}'.format(\n",
    "                phase, epoch_loss, epoch_acc))\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_acc > best_acc:\n",
    "                best_acc = epoch_acc\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "\n",
    "        print()\n",
    "\n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "        time_elapsed // 60, time_elapsed % 60))\n",
    "    print('Best val Acc: {:4f}'.format(best_acc))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visualizing the model predictions\n",
    "^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "Generic function to display predictions for a few images\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_model(model, num_images=6):\n",
    "    was_training = model.training\n",
    "    model.eval()\n",
    "    images_so_far = 0\n",
    "    fig = plt.figure()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i, (inputs, labels) in enumerate(dataloaders['val']):\n",
    "            inputs = inputs.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "\n",
    "            for j in range(inputs.size()[0]):\n",
    "                images_so_far += 1\n",
    "                ax = plt.subplot(num_images//2, 2, images_so_far)\n",
    "                ax.axis('off')\n",
    "                ax.set_title('predicted: {}'.format(class_names[preds[j]]))\n",
    "                imshow(inputs.cpu().data[j])\n",
    "\n",
    "                if images_so_far == num_images:\n",
    "                    model.train(mode=was_training)\n",
    "                    return\n",
    "        model.train(mode=was_training)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "ConvNet as fixed feature extractor\n",
    "----------------------------------\n",
    "\n",
    "Here, we need to freeze all the network except the final layer. We need\n",
    "to set ``requires_grad == False`` to freeze the parameters so that the\n",
    "gradients are not computed in ``backward()``.\n",
    "\n",
    "You can read more about this in the documentation\n",
    "`here <http://pytorch.org/docs/notes/autograd.html#excluding-subgraphs-from-backward>`__.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_conv = torchvision.models.resnet18(pretrained=True)\n",
    "for param in model_conv.parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Parameters of newly constructed modules have requires_grad=True by default\n",
    "num_ftrs = model_conv.fc.in_features\n",
    "model_conv.fc = nn.Linear(num_ftrs, 2)\n",
    "\n",
    "model_conv = model_conv.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Observe that only parameters of final layer are being optimized as\n",
    "# opoosed to before.\n",
    "optimizer_conv = optim.SGD(model_conv.fc.parameters(), lr=0.001, momentum=0.9)\n",
    "\n",
    "# Decay LR by a factor of 0.1 every 7 epochs\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer_conv, step_size=7, gamma=0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate\n",
    "^^^^^^^^^^^^^^^^^^\n",
    "\n",
    "On CPU this will take about half the time compared to previous scenario.\n",
    "This is expected as gradients don't need to be computed for most of the\n",
    "network. However, forward does need to be computed.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 0/24\n",
      "----------\n",
      "train Loss: 0.7053 Acc: 0.6230\n",
      "val Loss: 0.2214 Acc: 0.9281\n",
      "\n",
      "Epoch 1/24\n",
      "----------\n",
      "train Loss: 0.3399 Acc: 0.8156\n",
      "val Loss: 0.2148 Acc: 0.9216\n",
      "\n",
      "Epoch 2/24\n",
      "----------\n",
      "train Loss: 0.5299 Acc: 0.7869\n",
      "val Loss: 0.1898 Acc: 0.9346\n",
      "\n",
      "Epoch 3/24\n",
      "----------\n",
      "train Loss: 0.5668 Acc: 0.7500\n",
      "val Loss: 0.1675 Acc: 0.9477\n",
      "\n",
      "Epoch 4/24\n",
      "----------\n",
      "train Loss: 0.4072 Acc: 0.8279\n",
      "val Loss: 0.2490 Acc: 0.9150\n",
      "\n",
      "Epoch 5/24\n",
      "----------\n",
      "train Loss: 0.4953 Acc: 0.7910\n",
      "val Loss: 0.2045 Acc: 0.9346\n",
      "\n",
      "Epoch 6/24\n",
      "----------\n",
      "train Loss: 0.4391 Acc: 0.8156\n",
      "val Loss: 0.2586 Acc: 0.9281\n",
      "\n",
      "Epoch 7/24\n",
      "----------\n",
      "train Loss: 0.2935 Acc: 0.8730\n",
      "val Loss: 0.1821 Acc: 0.9412\n",
      "\n",
      "Epoch 8/24\n",
      "----------\n",
      "train Loss: 0.3394 Acc: 0.8525\n",
      "val Loss: 0.1751 Acc: 0.9281\n",
      "\n",
      "Epoch 9/24\n",
      "----------\n",
      "train Loss: 0.3197 Acc: 0.8648\n",
      "val Loss: 0.1766 Acc: 0.9477\n",
      "\n",
      "Epoch 10/24\n",
      "----------\n",
      "train Loss: 0.2754 Acc: 0.8770\n",
      "val Loss: 0.1810 Acc: 0.9412\n",
      "\n",
      "Epoch 11/24\n",
      "----------\n",
      "train Loss: 0.2934 Acc: 0.8689\n",
      "val Loss: 0.1954 Acc: 0.9281\n",
      "\n",
      "Epoch 12/24\n",
      "----------\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Process Process-104:\n",
      "Process Process-103:\n",
      "Process Process-102:\n",
      "Process Process-101:\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "Traceback (most recent call last):\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 258, in _bootstrap\n",
      "    self.run()\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/process.py\", line 93, in run\n",
      "    self._target(*self._args, **self._kwargs)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/site-packages/torch/utils/data/dataloader.py\", line 96, in _worker_loop\n",
      "    r = index_queue.get(timeout=MANAGER_STATUS_CHECK_INTERVAL)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/queues.py\", line 104, in get\n",
      "    if not self._poll(timeout):\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 257, in poll\n",
      "    return self._poll(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 414, in _poll\n",
      "    r = wait([self], timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/multiprocessing/connection.py\", line 911, in wait\n",
      "    ready = selector.select(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "  File \"/Users/signapoop/anaconda3/envs/py36/lib/python3.6/selectors.py\", line 376, in select\n",
      "    fd_event_list = self._poll.poll(timeout)\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n",
      "KeyboardInterrupt\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-8-6fbf470179af>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m model_conv = train_model(model_conv, criterion, optimizer_conv,\n\u001b[0;32m----> 2\u001b[0;31m                          exp_lr_scheduler, num_epochs=25)\n\u001b[0m",
      "\u001b[0;32m<ipython-input-5-77b76ce2b63b>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, criterion, optimizer, scheduler, num_epochs)\u001b[0m\n\u001b[1;32m     31\u001b[0m                 \u001b[0;31m# track history if only in train\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m                 \u001b[0;32mwith\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_grad_enabled\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mphase\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 33\u001b[0;31m                     \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     34\u001b[0m                     \u001b[0m_\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     35\u001b[0m                     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m    143\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer1\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 145\u001b[0;31m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    146\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer3\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    147\u001b[0m         \u001b[0mx\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlayer4\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torchvision-0.2.1-py3.6.egg/torchvision/models/resnet.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, x)\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrelu\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconv2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m         \u001b[0mout\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbn2\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/conv.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    299\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    300\u001b[0m         return F.conv2d(input, self.weight, self.bias, self.stride,\n\u001b[0;32m--> 301\u001b[0;31m                         self.padding, self.dilation, self.groups)\n\u001b[0m\u001b[1;32m    302\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    303\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "model_conv = train_model(model_conv, criterion, optimizer_conv,\n",
    "                         exp_lr_scheduler, num_epochs=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_model(model_conv)\n",
    "\n",
    "plt.ioff()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 224, 224])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Split: train\n",
       "    Root Location: /Users/signapoop/Desktop/data/\n",
       "    Transforms (if any): Compose(\n",
       "                             RandomCrop(size=(32, 32), padding=4)\n",
       "                             RandomHorizontalFlip(p=0.5)\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['train']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset CIFAR10\n",
       "    Number of datapoints: 50000\n",
       "    Split: train\n",
       "    Root Location: /Users/signapoop/Desktop/data/\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "image_datasets['val']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyModel(nn.Module):\n",
    "    def __init__(self, pretrained_model):\n",
    "        super(MyModel, self).__init__()\n",
    "        image_modules = list(pretrained_model.children())[:-1] #all layer expect last layer\n",
    "        self.modelA = nn.Sequential(*image_modules)\n",
    "        \n",
    "    def forward(self, image):\n",
    "        a = self.modelA(image)\n",
    "        return a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyModel(model_conv)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 3, 32, 32])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inputs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Given input size: (512x1x1). Calculated output size: (512x-5x-5). Output size is too small at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/THNN/generic/SpatialAveragePooling.c:64",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-30-e51f6a12482f>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mconv_features\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-15-35903f4cd378>\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, image)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 8\u001b[0;31m         \u001b[0ma\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmodelA\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      9\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0ma\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/container.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m     89\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     90\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mmodule\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_modules\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 91\u001b[0;31m             \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodule\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     92\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     93\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m    475\u001b[0m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_slow_forward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    476\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 477\u001b[0;31m             \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    478\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mhook\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_forward_hooks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    479\u001b[0m             \u001b[0mhook_result\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhook\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/envs/py36/lib/python3.6/site-packages/torch/nn/modules/pooling.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    545\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    546\u001b[0m         return F.avg_pool2d(input, self.kernel_size, self.stride,\n\u001b[0;32m--> 547\u001b[0;31m                             self.padding, self.ceil_mode, self.count_include_pad)\n\u001b[0m\u001b[1;32m    548\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    549\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: Given input size: (512x1x1). Calculated output size: (512x-5x-5). Output size is too small at /Users/soumith/miniconda2/conda-bld/pytorch_1532623076075/work/aten/src/THNN/generic/SpatialAveragePooling.c:64"
     ]
    }
   ],
   "source": [
    "conv_features = model(inputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([4, 512, 1, 1])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conv_features.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
