{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import time\n",
    "import pickle \n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "import pathlib\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.GraphConvNet2 import GraphConvNet2\n",
    "from core.DataEmbeddingGraph import DataEmbeddingGraph\n",
    "from util.plot_embedding import plot_embedding, plot_embedding_subplot\n",
    "from util.evaluation_metrics import evaluate_net_metrics, evaluate_embedding_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    device = 'cuda'\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "net_parameters = {}\n",
    "net_parameters['n_components'] = 2\n",
    "net_parameters['D'] = 784 # input dimension\n",
    "net_parameters['H'] = 50 # number of hidden units\n",
    "net_parameters['L'] = 10 # number of hidden layers\n",
    "net_parameters['n_channels'] = 1\n",
    "net_parameters['n_units_1'] = net_parameters['n_units_2'] = net_parameters['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graph net\n",
    "net = GraphConvNet2(net_parameters)\n",
    "root = 'results/mnist_preprocessed_tsne1/'\n",
    "filename = root + 'graph_net5.pkl'\n",
    "checkpoint = torch.load(filename, map_location=device)\n",
    "net.load_state_dict(checkpoint['state_dict'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Freeze all top layers\n",
    "for param in net.parameters():\n",
    "    param.requires_grad = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Linear(in_features=50, out_features=2, bias=True)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.fc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset final layer\n",
    "net.fc = nn.Linear(net.fc.in_features, net.fc.out_features)\n",
    "net = net.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_parameters = {}\n",
    "task_parameters['net_type'] = 'graph_net'\n",
    "task_parameters['reduction_method'] = 'preprocessed_tsne'\n",
    "task_parameters['loss_function'] = 'pairwise_loss'\n",
    "task_parameters['n_components'] = 2\n",
    "\n",
    "# optimization parameters\n",
    "opt_parameters = {}\n",
    "opt_parameters['learning_rate'] = 0.00075   # ADAM\n",
    "opt_parameters['max_iters'] = 100   \n",
    "opt_parameters['batch_iters'] = 10\n",
    "opt_parameters['max_train_size'] = 5000   \n",
    "opt_parameters['save_flag'] = True\n",
    "\n",
    "if 2==1: # fast debugging\n",
    "    opt_parameters['max_iters'] = 5 \n",
    "    opt_parameters['batch_iters'] = 1\n",
    "    opt_parameters['save_flag'] = False\n",
    "\n",
    "opt_parameters['decay_rate'] = 1.25  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt_parameters['save_flag']: \n",
    "    checkpoint_interval = opt_parameters['max_iters']/5\n",
    "    checkpoint_root = 'results/usps_' + task_parameters['reduction_method'] + '2/'\n",
    "    pathlib.Path(checkpoint_root).mkdir(exist_ok=True) # create the directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processed train data\n",
    "parent_dir = os.path.abspath('..')\n",
    "filename = parent_dir + '/data/set_5000_usps_tsne.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    [inputs, labels, X_emb] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data = []\n",
    "num_train_samples = 0\n",
    "while num_train_samples <= opt_parameters['max_train_size']:\n",
    "    # Draw a random training batch of variable size\n",
    "    num_samples = np.random.randint(200, 500)\n",
    "    inputs_subset = inputs[num_train_samples:num_train_samples+num_samples]\n",
    "    labels_subset = labels[num_train_samples:num_train_samples+num_samples]\n",
    "    X_emb_subset = X_emb[num_train_samples:num_train_samples+num_samples]\n",
    "    \n",
    "    # Package into graph block\n",
    "    G = DataEmbeddingGraph(inputs_subset, labels_subset, 'spectral')\n",
    "    G.target = X_emb_subset\n",
    "    \n",
    "    all_train_data.append(G)\n",
    "    num_train_samples += num_samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes = [223, 326, 492, 493, 249, 208, 484, 291, 301, 413, 345, 359, 232, 449, 135]\n",
      "Total number of samples = 5000\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = [len(G.labels) for G in all_train_data]\n",
    "print(\"Dataset sizes = {}\".format(dataset_sizes))\n",
    "print(\"Total number of samples = {}\".format(sum(dataset_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "filename = parent_dir + '/data/set_100_usps_spectral_size_200_500.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    [all_test_data] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leowyaoyang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:40: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration= 0, loss(10iter)= 1317.97546387, lr= 0.00075000, time(10iter)= 3.15\n",
      "Trust = 0.7165, 1-NN = 0.3925, time to compute = 0.22s\n",
      "\n",
      "iteration= 10, loss(10iter)= 924.78784180, lr= 0.00075000, time(10iter)= 49.72\n",
      "Trust = 0.7107, 1-NN = 0.3874, time to compute = 0.21s\n",
      "\n",
      "iteration= 20, loss(10iter)= 661.02783203, lr= 0.00075000, time(10iter)= 47.75\n",
      "Trust = 0.7085, 1-NN = 0.3852, time to compute = 0.21s\n",
      "\n",
      "iteration= 30, loss(10iter)= 634.25439453, lr= 0.00075000, time(10iter)= 48.84\n",
      "Trust = 0.7071, 1-NN = 0.3823, time to compute = 0.21s\n",
      "\n",
      "iteration= 40, loss(10iter)= 629.85632324, lr= 0.00060000, time(10iter)= 48.43\n",
      "Trust = 0.7062, 1-NN = 0.3830, time to compute = 0.21s\n",
      "\n",
      "iteration= 50, loss(10iter)= 626.72802734, lr= 0.00048000, time(10iter)= 48.60\n",
      "Trust = 0.7052, 1-NN = 0.3800, time to compute = 0.21s\n",
      "\n",
      "iteration= 60, loss(10iter)= 624.22650146, lr= 0.00038400, time(10iter)= 49.03\n",
      "Trust = 0.7039, 1-NN = 0.3795, time to compute = 0.20s\n",
      "\n",
      "iteration= 70, loss(10iter)= 622.21728516, lr= 0.00030720, time(10iter)= 47.81\n",
      "Trust = 0.7032, 1-NN = 0.3787, time to compute = 0.21s\n",
      "\n",
      "iteration= 80, loss(10iter)= 620.60247803, lr= 0.00024576, time(10iter)= 48.32\n",
      "Trust = 0.7026, 1-NN = 0.3785, time to compute = 0.21s\n",
      "\n",
      "iteration= 90, loss(10iter)= 619.30102539, lr= 0.00019661, time(10iter)= 48.49\n",
      "Trust = 0.7020, 1-NN = 0.3772, time to compute = 0.21s\n"
     ]
    }
   ],
   "source": [
    "# Optimization parameters\n",
    "learning_rate = opt_parameters['learning_rate']\n",
    "max_iters = opt_parameters['max_iters']\n",
    "batch_iters = opt_parameters['batch_iters']\n",
    "decay_rate = opt_parameters['decay_rate']\n",
    "\n",
    "# Optimizer\n",
    "global_lr = learning_rate\n",
    "global_step = 0\n",
    "lr = learning_rate\n",
    "optimizer = net.update(lr) \n",
    "\n",
    "# Statistics\n",
    "t_start = time.time()\n",
    "t_start_total = time.time()\n",
    "average_loss_old = 1e10\n",
    "running_loss = 0.0\n",
    "running_total = 0\n",
    "tab_results = []\n",
    "    \n",
    "for iteration in range(max_iters):\n",
    "    net.train()\n",
    "    \n",
    "    for G in all_train_data:\n",
    "        # Forward pass\n",
    "        y_pred = net.forward(G)\n",
    "\n",
    "        # Target embedding matrix\n",
    "        y_true = G.target\n",
    "        y_true = Variable(torch.FloatTensor(y_true).type(dtypeFloat) , requires_grad=False) \n",
    "\n",
    "        # Compute overall loss \n",
    "        if task_parameters['loss_function'] == 'pairwise_loss':\n",
    "            loss = net.pairwise_loss(y_pred, y_true, G.adj_matrix)\n",
    "        elif task_parameters['loss_function'] == 'composite_loss':\n",
    "            loss1 = net.loss(y_pred, y_true)\n",
    "            loss2 = net.pairwise_loss(y_pred, y_true, G.adj_matrix)\n",
    "            loss = 0.5*loss1 + 0.5*loss2\n",
    "            \n",
    "        loss_train = loss.data[0]\n",
    "        running_loss += loss_train\n",
    "        running_total += 1\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # learning rate, print results\n",
    "    if not iteration%batch_iters:\n",
    "\n",
    "        # time\n",
    "        t_stop = time.time() - t_start\n",
    "        t_start = time.time()\n",
    "\n",
    "        # update learning rate \n",
    "        average_loss = running_loss/ running_total\n",
    "        if average_loss > 0.99* average_loss_old:\n",
    "            lr /= decay_rate\n",
    "        average_loss_old = average_loss\n",
    "        optimizer = net.update_learning_rate(optimizer, lr)\n",
    "        running_loss = 0.0\n",
    "        running_total = 0\n",
    "\n",
    "        # print results\n",
    "        print('\\niteration= %d, loss(%diter)= %.8f, lr= %.8f, time(%diter)= %.2f' %\n",
    "              (iteration, batch_iters, average_loss, lr, batch_iters, t_stop))\n",
    "            \n",
    "        # validate on test set\n",
    "        #######################\n",
    "        trustworthiness, one_nn, time_elapsed = evaluate_net_metrics(all_test_data, net)\n",
    "        print(\"Trust = {:.4f}, 1-NN = {:.4f}, time to compute = {:.2f}s\".format(trustworthiness, one_nn, time_elapsed))\n",
    "        \n",
    "        tab_results.append([iteration, average_loss, trustworthiness, one_nn, time.time()-t_start_total])\n",
    "    \n",
    "    if opt_parameters['save_flag'] and not (iteration+1)%checkpoint_interval:\n",
    "        filename = checkpoint_root + task_parameters['net_type'] + str(int((iteration+1)/checkpoint_interval)) + '.pkl'\n",
    "        save_checkpoint({\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
