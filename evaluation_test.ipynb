{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import pickle \n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "from timeit import default_timer as timer\n",
    "\n",
    "from sklearn import manifold\n",
    "from sklearn.manifold.t_sne import trustworthiness\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn import neighbors\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.SimpleConvNet import SimpleConvNet\n",
    "from core.GraphConvNet2 import GraphConvNet2\n",
    "from core.DataEmbeddingGraph import DataEmbeddingGraph\n",
    "from util.plot_embedding import plot_embedding, plot_embedding_subplot\n",
    "from util.mnist_data_loader import get_train_set, get_test_set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda not available\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    device = 'gpu'\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    device = 'cpu'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_parameters = {}\n",
    "task_parameters['reduction_method'] = 'spectral'\n",
    "task_parameters['n_components'] = 2\n",
    "\n",
    "net_parameters = {}\n",
    "net_parameters['n_components'] = task_parameters['n_components']\n",
    "net_parameters['D'] = 784 # input dimension\n",
    "net_parameters['H'] = 50 # number of hidden units\n",
    "net_parameters['L'] = 10 # number of hidden layers\n",
    "net_parameters['n_channels'] = 1\n",
    "net_parameters['n_units_1'] = net_parameters['n_units_2'] = net_parameters['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "filename = 'data/set_100_mnist_spectral_size_200_500.pkl'\n",
    "with open(filename, 'rb') as f:\n",
    "    [all_test_data] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss functions\n",
    "def l2_norm(v):\n",
    "    return np.sqrt(np.sum(np.square(v), axis=1, dtype=np.float64))\n",
    "\n",
    "def pairwise_loss_function_1(y_true, y_pred, W):\n",
    "    pairwise_loss_1 = mean_squared_error(y_true[W.row,:], y_true[W.col,:])\n",
    "    pairwise_loss_2 = mean_squared_error(y_pred[W.row,:], y_pred[W.col,:])\n",
    "    return np.square(pairwise_loss_1 - pairwise_loss_2)\n",
    "\n",
    "def pairwise_loss_function_2(y_true, y_pred, W):\n",
    "    pairwise_loss_1 = l2_norm(y_true[W.row,:] - y_true[W.col,:])\n",
    "    pairwise_loss_2 = l2_norm(y_pred[W.row,:] - y_pred[W.col,:])\n",
    "    return np.average(np.square(pairwise_loss_1 - pairwise_loss_2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_nearest_neighbours_generalisation_error(X, y):\n",
    "    kf = KFold(n_splits=10)\n",
    "    kf.get_n_splits(X)\n",
    "    errors = []\n",
    "    for train_index, test_index in kf.split(X):\n",
    "        X_train, X_test = X[train_index], X[test_index]\n",
    "        y_train, y_test = y[train_index], y[test_index]\n",
    "        clf = neighbors.KNeighborsClassifier(n_neighbors=1)\n",
    "        clf.fit(X_train, y_train)\n",
    "        y_pred = clf.predict(X_test)\n",
    "        errors.append(1- accuracy_score(y_test, y_pred))\n",
    "    return np.average(errors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_net(net, all_test_data):\n",
    "    n_test = len(all_test_data)\n",
    "    similarity_tracker = np.zeros((n_test, ))\n",
    "    trust_tracker = np.zeros((n_test, ))\n",
    "    one_nn_tracker = np.zeros((n_test, ))\n",
    "    time_tracker = np.zeros((n_test, ))\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        G = all_test_data[i]\n",
    "        time_start = timer()\n",
    "        y_pred = net.forward(G).detach().numpy()\n",
    "        time_end = timer()\n",
    "        time_tracker[i] = time_end - time_start\n",
    "        \n",
    "        similarity_tracker[i] =  pairwise_loss_function_2(G.target, y_pred, G.adj_matrix)\n",
    "        X = G.data.view(G.data.shape[0], -1).numpy()\n",
    "        trust_tracker[i] = trustworthiness(X, y_pred, n_neighbors=5)\n",
    "        one_nn_tracker[i] = one_nearest_neighbours_generalisation_error(y_pred, G.labels.numpy())\n",
    "    return similarity_tracker, trust_tracker, one_nn_tracker, time_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_original_embedding(all_test_data):\n",
    "    n_test = len(all_test_data)\n",
    "    trust_tracker = np.zeros((n_test, ))\n",
    "    one_nn_tracker = np.zeros((n_test, ))\n",
    "    time_tracker = np.zeros((n_test, )) \n",
    "    \n",
    "    for i in range(n_test):\n",
    "        G = all_test_data[i]\n",
    "        X = G.data.view(G.data.shape[0], -1).numpy()\n",
    "        time_start = timer()\n",
    "        embedder = manifold.SpectralEmbedding(n_components=2, random_state=0,\n",
    "                                              eigen_solver=\"arpack\")\n",
    "        X_spectral = embedder.fit_transform(X)\n",
    "        time_end = timer()\n",
    "        time_tracker[i] = time_end - time_start\n",
    "        \n",
    "        trust_tracker[i] = trustworthiness(X, X_spectral, n_neighbors=5)\n",
    "        one_nn_tracker[i] = one_nearest_neighbours_generalisation_error(X_spectral, G.labels.numpy())\n",
    "    return trust_tracker, one_nn_tracker, time_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_tsne_embedding(all_test_data):\n",
    "    n_test = len(all_test_data)\n",
    "    trust_tracker = np.zeros((n_test, ))\n",
    "    one_nn_tracker = np.zeros((n_test, ))\n",
    "    time_tracker = np.zeros((n_test, ))\n",
    "    \n",
    "    for i in range(n_test):\n",
    "        G = all_test_data[i]\n",
    "        X = G.data.view(G.data.shape[0], -1).numpy()\n",
    "        time_start = timer()\n",
    "        embedder = manifold.TSNE(n_components=2, init='pca', random_state=0)\n",
    "        X_tsne = embedder.fit_transform(X)\n",
    "        time_end = timer()\n",
    "        time_tracker[i] = time_end - time_start\n",
    "        \n",
    "        trust_tracker[i] = trustworthiness(X, X_tsne, n_neighbors=5)\n",
    "        one_nn_tracker[i] = one_nearest_neighbours_generalisation_error(X_tsne, G.labels.numpy())\n",
    "    return trust_tracker, one_nn_tracker, time_tracker"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trustworthy score = 0.7582520548568904\n",
      "1-NN generalisation error = 0.5524332002099447\n",
      "Average time to compute (s) = 0.17640546113987512\n"
     ]
    }
   ],
   "source": [
    "# Spectral embeddings\n",
    "trust_tracker, one_nn_tracker, time_tracker = evaluate_original_embedding(all_test_data)\n",
    "print(\"Trustworthy score = {}\".format(np.average(trust_tracker)))\n",
    "print(\"1-NN generalisation error = {}\".format(np.average(one_nn_tracker)))\n",
    "print(\"Average time to compute (s) = {}\".format(np.average(time_tracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trustworthy score = 0.955044333450673\n",
      "1-NN generalisation error = 0.18022943449431011\n",
      "Average time to compute (s) = 5.86404038087021\n"
     ]
    }
   ],
   "source": [
    "# t-SNE embeddings\n",
    "trust_tracker, one_nn_tracker, time_tracker = evaluate_tsne_embedding(all_test_data)\n",
    "print(\"Trustworthy score = {}\".format(np.average(trust_tracker)))\n",
    "print(\"1-NN generalisation error = {}\".format(np.average(one_nn_tracker)))\n",
    "print(\"Average time to compute (s) = {}\".format(np.average(time_tracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity loss = 0.0709895799395694\n",
      "Trustworthy score = 0.5501872305425336\n",
      "1-NN generalisation error = 0.8476193517441856\n",
      "Average time to compute (s) = 0.7939383930801704\n"
     ]
    }
   ],
   "source": [
    "# Simple conv net\n",
    "net = SimpleConvNet(net_parameters)\n",
    "root = 'results/mnist_spectral1/'\n",
    "filename = root + 'conv_net5.pkl'\n",
    "checkpoint = torch.load(filename, map_location=device)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "similarity_tracker, trust_tracker, one_nn_tracker, time_tracker = evaluate_net(net, all_test_data)\n",
    "print(\"Similarity loss = {}\".format(np.average(similarity_tracker)))\n",
    "print(\"Trustworthy score = {}\".format(np.average(trust_tracker)))\n",
    "print(\"1-NN generalisation error = {}\".format(np.average(one_nn_tracker)))\n",
    "print(\"Average time to compute (s) = {}\".format(np.average(time_tracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity loss = 214.97720496898555\n",
      "Trustworthy score = 0.771702325260192\n",
      "1-NN generalisation error = 0.5683302793371392\n",
      "Average time to compute (s) = 0.370508154551062\n"
     ]
    }
   ],
   "source": [
    "# Graph net\n",
    "net = GraphConvNet2(net_parameters)\n",
    "root = 'results/mnist_tsne3/'\n",
    "filename = root + 'graph_net5.pkl'\n",
    "checkpoint = torch.load(filename, map_location=device)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "similarity_tracker, trust_tracker, one_nn_tracker, time_tracker = evaluate_net(net, all_test_data)\n",
    "print(\"Similarity loss = {}\".format(np.average(similarity_tracker)))\n",
    "print(\"Trustworthy score = {}\".format(np.average(trust_tracker)))\n",
    "print(\"1-NN generalisation error = {}\".format(np.average(one_nn_tracker)))\n",
    "print(\"Average time to compute (s) = {}\".format(np.average(time_tracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch.autograd import Variable\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "from core.GraphConvNetCell import GraphConvNetCell\n",
    "\n",
    "from sklearn import decomposition\n",
    "\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "else:\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor\n",
    "\n",
    "\n",
    "class GraphConvNet3(nn.Module):\n",
    "\n",
    "    def __init__(self, net_parameters):\n",
    "\n",
    "        super(GraphConvNet3, self).__init__()\n",
    "\n",
    "        # parameters\n",
    "        D = net_parameters['D']\n",
    "        n_components = net_parameters['n_components']\n",
    "        H = net_parameters['H']\n",
    "        L = net_parameters['L']\n",
    "\n",
    "        # vector of hidden dimensions\n",
    "        net_layers = []\n",
    "        for layer in range(L):\n",
    "            net_layers.append(H)\n",
    "\n",
    "        # CL cells\n",
    "        # NOTE: Each graph convnet cell uses *TWO* convolutional operations\n",
    "        net_layers_extended = [D] + net_layers  # include embedding dim\n",
    "        L = len(net_layers)\n",
    "        list_of_gnn_cells = []  # list of NN cells\n",
    "        for layer in range(L // 2):\n",
    "            Hin, Hout = net_layers_extended[2 * layer], net_layers_extended[2 * layer + 2]\n",
    "            list_of_gnn_cells.append(GraphConvNetCell(Hin, Hout))\n",
    "\n",
    "        # register the cells for pytorch\n",
    "        self.gnn_cells = nn.ModuleList(list_of_gnn_cells)\n",
    "\n",
    "        # fc\n",
    "        Hfinal = net_layers_extended[-1]\n",
    "        self.fc = nn.Linear(Hfinal, n_components)\n",
    "\n",
    "        # init\n",
    "        self.init_weights_Graph_OurConvNet(Hfinal, n_components, 1)\n",
    "\n",
    "        # print('\\nnb of hidden layers=', L)\n",
    "        # print('dim of layers (w/ embed dim)=', net_layers_extended)\n",
    "        # print('\\n')\n",
    "\n",
    "        # class variables\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.net_layers_extended = net_layers_extended\n",
    "\n",
    "    def init_weights_Graph_OurConvNet(self, Fin_fc, Fout_fc, gain):\n",
    "\n",
    "        scale = gain * np.sqrt(2.0 / Fin_fc)\n",
    "        self.fc.weight.data.uniform_(-scale, scale)\n",
    "        self.fc.bias.data.fill_(0)\n",
    "\n",
    "    def forward(self, G):\n",
    "        # Data matrix\n",
    "        x = G.data\n",
    "\n",
    "        # Unroll the image vector\n",
    "        x = x.view(x.shape[0], -1)\n",
    "        \n",
    "        x = decomposition.TruncatedSVD(n_components=self.D).fit_transform(x)\n",
    "\n",
    "        # Pass raw data matrix X directly as input\n",
    "        x = Variable(torch.FloatTensor(x).type(dtypeFloat), requires_grad=False)\n",
    "\n",
    "        # graph operators\n",
    "        # Edge = start vertex to end vertex\n",
    "        # E_start = E x V mapping matrix from edge index to corresponding start vertex\n",
    "        # E_end = E x V mapping matrix from edge index to corresponding end vertex\n",
    "        E_start = G.edge_to_starting_vertex\n",
    "        E_end = G.edge_to_ending_vertex\n",
    "        E_start = torch.from_numpy(E_start.toarray()).type(dtypeFloat)\n",
    "        E_end = torch.from_numpy(E_end.toarray()).type(dtypeFloat)\n",
    "        E_start = Variable(E_start, requires_grad=False)\n",
    "        E_end = Variable(E_end, requires_grad=False)\n",
    "\n",
    "        for layer in range(self.L // 2):\n",
    "            gnn_layer = self.gnn_cells[layer]\n",
    "            x = gnn_layer(x, E_start, E_end)  # V x Hfinal\n",
    "\n",
    "        # FC\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def loss(self, y, y_target):\n",
    "        # L2 loss\n",
    "        loss = nn.MSELoss()(y, y_target)\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def pairwise_loss(self, y, y_target, W):\n",
    "        distances_1 = y_target[W.row, :] - y_target[W.col, :]\n",
    "        distances_2 = y[W.row, :] - y[W.col, :]\n",
    "        loss = torch.mean(torch.pow(distances_1.norm(dim=1) - distances_2.norm(dim=1), 2))\n",
    "\n",
    "        return loss\n",
    "\n",
    "    def update(self, lr):\n",
    "\n",
    "        update = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        return update\n",
    "\n",
    "    def update_learning_rate(self, optimizer, lr):\n",
    "\n",
    "        for param_group in optimizer.param_groups:\n",
    "            param_group['lr'] = lr\n",
    "\n",
    "        return optimizer\n",
    "\n",
    "    def nb_param(self):\n",
    "        return self.nb_param\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity loss = 239.26064496621643\n",
      "Trustworthy score = 0.7155905432916881\n",
      "1-NN generalisation error = 0.6684939197182875\n",
      "Average time to compute (s) = 0.36324966807973397\n"
     ]
    }
   ],
   "source": [
    "# Graph net\n",
    "net = GraphConvNet3(net_parameters)\n",
    "root = 'results/mnist_tsne4_pca/'\n",
    "filename = root + 'graph_net5.pkl'\n",
    "checkpoint = torch.load(filename, map_location=device)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "similarity_tracker, trust_tracker, one_nn_tracker, time_tracker = evaluate_net(net, all_test_data)\n",
    "print(\"Similarity loss = {}\".format(np.average(similarity_tracker)))\n",
    "print(\"Trustworthy score = {}\".format(np.average(trust_tracker)))\n",
    "print(\"1-NN generalisation error = {}\".format(np.average(one_nn_tracker)))\n",
    "print(\"Average time to compute (s) = {}\".format(np.average(time_tracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Similarity loss = 314.02360915456586\n",
      "Trustworthy score = 0.5195835651987438\n",
      "1-NN generalisation error = 0.8855259485903647\n",
      "Average time to compute (s) = 0.6071047530096257\n"
     ]
    }
   ],
   "source": [
    "# Simple conv net\n",
    "net = SimpleConvNet(net_parameters)\n",
    "root = 'results/mnist_tsne2/'\n",
    "filename = root + 'conv_net5.pkl'\n",
    "checkpoint = torch.load(filename, map_location=device)\n",
    "net.load_state_dict(checkpoint['state_dict'])\n",
    "\n",
    "similarity_tracker, trust_tracker, one_nn_tracker, time_tracker = evaluate_net(net, all_test_data)\n",
    "print(\"Similarity loss = {}\".format(np.average(similarity_tracker)))\n",
    "print(\"Trustworthy score = {}\".format(np.average(trust_tracker)))\n",
    "print(\"1-NN generalisation error = {}\".format(np.average(one_nn_tracker)))\n",
    "print(\"Average time to compute (s) = {}\".format(np.average(time_tracker)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (py36)",
   "language": "python",
   "name": "py36"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
