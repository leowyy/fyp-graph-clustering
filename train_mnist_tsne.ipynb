{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.SimpleConvNet import SimpleConvNet\n",
    "from core.GraphConvNet2 import GraphConvNet2\n",
    "from core.DataEmbeddingGraph import DataEmbeddingGraph\n",
    "from util.mnist_data_loader import get_train_set, get_test_set\n",
    "from util.plot_embedding import plot_embedding, plot_embedding_subplot\n",
    "from util.draw_random_subset import draw_random_subset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_parameters = {}\n",
    "task_parameters['conv_type'] = 'graph_net'\n",
    "task_parameters['reduction_method'] = 'tsne'\n",
    "task_parameters['n_components'] = 2\n",
    "\n",
    "net_parameters = {}\n",
    "net_parameters['n_components'] = task_parameters['n_components']\n",
    "net_parameters['D'] = 784 # input dimension\n",
    "net_parameters['H'] = 50 # number of hidden units\n",
    "net_parameters['L'] = 10 # number of hidden layers\n",
    "net_parameters['n_channels'] = 1\n",
    "net_parameters['n_units_1'] = net_parameters['n_units_2'] = net_parameters['H']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GraphConvNet2(\n",
      "  (gnn_cells): ModuleList(\n",
      "    (0): GraphConvNetCell(\n",
      "      (Ui1): Linear(in_features=784, out_features=50, bias=False)\n",
      "      (Uj1): Linear(in_features=784, out_features=50, bias=False)\n",
      "      (Vi1): Linear(in_features=784, out_features=50, bias=False)\n",
      "      (Vj1): Linear(in_features=784, out_features=50, bias=False)\n",
      "      (Ui2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (R): Linear(in_features=784, out_features=50, bias=False)\n",
      "    )\n",
      "    (1): GraphConvNetCell(\n",
      "      (Ui1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Ui2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (R): Linear(in_features=50, out_features=50, bias=False)\n",
      "    )\n",
      "    (2): GraphConvNetCell(\n",
      "      (Ui1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Ui2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (R): Linear(in_features=50, out_features=50, bias=False)\n",
      "    )\n",
      "    (3): GraphConvNetCell(\n",
      "      (Ui1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Ui2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (R): Linear(in_features=50, out_features=50, bias=False)\n",
      "    )\n",
      "    (4): GraphConvNetCell(\n",
      "      (Ui1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj1): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Ui2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Uj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vi2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (Vj2): Linear(in_features=50, out_features=50, bias=False)\n",
      "      (bn1): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (bn2): BatchNorm1d(50, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "      (R): Linear(in_features=50, out_features=50, bias=False)\n",
      "    )\n",
      "  )\n",
      "  (fc): Linear(in_features=50, out_features=2, bias=True)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "if task_parameters['conv_type'] == 'graph_net':\n",
    "    net = GraphConvNet2(net_parameters)\n",
    "elif task_parameters['conv_type'] == 'conv_net':\n",
    "    net = SimpleConvNet(net_parameters)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "opt_parameters = {}\n",
    "opt_parameters['learning_rate'] = 0.00075   # ADAM\n",
    "opt_parameters['max_iters'] = 400   \n",
    "opt_parameters['max_train_size'] = 20000   \n",
    "opt_parameters['batch_iters'] = 10\n",
    "opt_parameters['save_flag'] = True\n",
    "\n",
    "if 2==1: # fast debugging\n",
    "    opt_parameters['max_iters'] = 25 \n",
    "    opt_parameters['batch_iters'] = 5\n",
    "    opt_parameters['save_flag'] = False\n",
    "\n",
    "opt_parameters['decay_rate'] = 1.25   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load Mnist dataset\n",
    "parent_dir = os.path.abspath('..')\n",
    "data_dir = parent_dir + '/data/mnist'\n",
    "train_data = get_train_set(data_dir)\n",
    "test_data = get_test_set(data_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt_parameters['save_flag']: \n",
    "    checkpoint_interval = opt_parameters['max_iters']/5\n",
    "    checkpoint_root = 'results/mnist_tsne6/'\n",
    "    pathlib.Path(checkpoint_root).mkdir(exist_ok=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset MNIST\n",
       "    Number of datapoints: 60000\n",
       "    Split: train\n",
       "    Root Location: /home/leowyaoyang/data/mnist\n",
       "    Transforms (if any): Compose(\n",
       "                             ToTensor()\n",
       "                             Normalize(mean=(0.1307,), std=(0.3081,))\n",
       "                         )\n",
       "    Target Transforms (if any): None"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Initialise train_loader based on batch size\n",
    "# batch_size = 200\n",
    "\n",
    "# train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size,\n",
    "#                                            num_workers=2, pin_memory=False)\n",
    "# test_loader = torch.utils.data.DataLoader(test_data, batch_size=batch_size, shuffle=True,\n",
    "#                                           num_workers=2, pin_memory=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data = []\n",
    "num_train_samples = 0\n",
    "while num_train_samples <= opt_parameters['max_train_size']:\n",
    "    # Draw a random training batch of variable size\n",
    "    num_samples = np.random.randint(200, 500)\n",
    "    inputs, labels = draw_random_subset(train_data, num_samples=num_samples)\n",
    "    \n",
    "    # Package into graph block\n",
    "    G = DataEmbeddingGraph(inputs, labels, task_parameters['reduction_method'])\n",
    "    \n",
    "    all_train_data.append(G)\n",
    "    num_train_samples += num_samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes = [231, 369, 335, 476, 325, 463, 325, 420, 217, 343, 336, 405, 374, 315, 347, 363, 213, 241, 389, 366, 293, 240, 262, 260, 428, 447, 240, 343, 233, 211, 251, 345, 419, 444, 249, 400, 289, 374, 232, 286, 447, 200, 206, 259, 329, 249, 328, 393, 310, 333, 368, 460, 336, 326, 482, 371, 294, 494, 360, 471]\n",
      "Total number of samples = 20115\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = [len(G.labels) for G in all_train_data]\n",
    "print(\"Dataset sizes = {}\".format(dataset_sizes))\n",
    "print(\"Total number of samples = {}\".format(num_train_samples))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leowyaoyang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:33: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration= 0, loss(10iter)= 191.69404602, lr= 0.00075000, time(10iter)= 11.48\n",
      "\n",
      "iteration= 10, loss(10iter)= 80.91379547, lr= 0.00075000, time(10iter)= 110.27\n",
      "\n",
      "iteration= 20, loss(10iter)= 64.85783386, lr= 0.00075000, time(10iter)= 112.33\n",
      "\n",
      "iteration= 30, loss(10iter)= 53.53060532, lr= 0.00075000, time(10iter)= 112.12\n",
      "\n",
      "iteration= 40, loss(10iter)= 46.77636337, lr= 0.00075000, time(10iter)= 112.04\n",
      "\n",
      "iteration= 50, loss(10iter)= 40.21768188, lr= 0.00075000, time(10iter)= 111.66\n",
      "\n",
      "iteration= 60, loss(10iter)= 35.51799393, lr= 0.00075000, time(10iter)= 111.25\n",
      "\n",
      "iteration= 70, loss(10iter)= 31.42062950, lr= 0.00075000, time(10iter)= 112.51\n",
      "\n",
      "iteration= 80, loss(10iter)= 26.40083694, lr= 0.00075000, time(10iter)= 111.67\n",
      "\n",
      "iteration= 90, loss(10iter)= 22.02286720, lr= 0.00075000, time(10iter)= 116.93\n",
      "\n",
      "iteration= 100, loss(10iter)= 19.94542122, lr= 0.00075000, time(10iter)= 116.90\n",
      "\n",
      "iteration= 110, loss(10iter)= 20.52236748, lr= 0.00060000, time(10iter)= 117.81\n",
      "\n",
      "iteration= 120, loss(10iter)= 15.76784992, lr= 0.00060000, time(10iter)= 118.20\n",
      "\n",
      "iteration= 130, loss(10iter)= 15.35556793, lr= 0.00060000, time(10iter)= 118.08\n",
      "\n",
      "iteration= 140, loss(10iter)= 13.86413002, lr= 0.00060000, time(10iter)= 117.63\n",
      "\n",
      "iteration= 150, loss(10iter)= 13.85177612, lr= 0.00048000, time(10iter)= 118.14\n",
      "\n",
      "iteration= 160, loss(10iter)= 11.24632740, lr= 0.00048000, time(10iter)= 118.04\n",
      "\n",
      "iteration= 170, loss(10iter)= 10.48438358, lr= 0.00048000, time(10iter)= 117.22\n",
      "\n",
      "iteration= 180, loss(10iter)= 11.33054543, lr= 0.00038400, time(10iter)= 117.80\n",
      "\n",
      "iteration= 190, loss(10iter)= 8.16258907, lr= 0.00038400, time(10iter)= 117.10\n",
      "\n",
      "iteration= 200, loss(10iter)= 8.72896290, lr= 0.00030720, time(10iter)= 117.75\n",
      "\n",
      "iteration= 210, loss(10iter)= 7.00601673, lr= 0.00030720, time(10iter)= 118.02\n",
      "\n",
      "iteration= 220, loss(10iter)= 7.16808844, lr= 0.00024576, time(10iter)= 117.73\n",
      "\n",
      "iteration= 230, loss(10iter)= 6.49176836, lr= 0.00024576, time(10iter)= 118.16\n",
      "\n",
      "iteration= 240, loss(10iter)= 6.18844557, lr= 0.00024576, time(10iter)= 118.65\n",
      "\n",
      "iteration= 250, loss(10iter)= 5.71047163, lr= 0.00024576, time(10iter)= 117.48\n",
      "\n",
      "iteration= 260, loss(10iter)= 5.48597813, lr= 0.00024576, time(10iter)= 118.18\n",
      "\n",
      "iteration= 270, loss(10iter)= 5.37446928, lr= 0.00024576, time(10iter)= 117.66\n",
      "\n",
      "iteration= 280, loss(10iter)= 5.31161070, lr= 0.00024576, time(10iter)= 117.74\n",
      "\n",
      "iteration= 290, loss(10iter)= 5.26670504, lr= 0.00019661, time(10iter)= 117.63\n",
      "\n",
      "iteration= 300, loss(10iter)= 4.41451645, lr= 0.00019661, time(10iter)= 117.85\n",
      "\n",
      "iteration= 310, loss(10iter)= 4.55417728, lr= 0.00015729, time(10iter)= 118.16\n",
      "\n",
      "iteration= 320, loss(10iter)= 4.08535767, lr= 0.00015729, time(10iter)= 117.99\n",
      "\n",
      "iteration= 330, loss(10iter)= 4.01808310, lr= 0.00015729, time(10iter)= 117.75\n",
      "\n",
      "iteration= 340, loss(10iter)= 4.16397953, lr= 0.00012583, time(10iter)= 117.56\n",
      "\n",
      "iteration= 350, loss(10iter)= 3.50808001, lr= 0.00012583, time(10iter)= 118.05\n",
      "\n",
      "iteration= 360, loss(10iter)= 3.34959126, lr= 0.00012583, time(10iter)= 119.44\n",
      "\n",
      "iteration= 370, loss(10iter)= 3.42345834, lr= 0.00010066, time(10iter)= 119.38\n",
      "\n",
      "iteration= 380, loss(10iter)= 3.18901300, lr= 0.00010066, time(10iter)= 119.07\n",
      "\n",
      "iteration= 390, loss(10iter)= 3.11567736, lr= 0.00010066, time(10iter)= 118.99\n"
     ]
    }
   ],
   "source": [
    "# Optimization parameters\n",
    "learning_rate = opt_parameters['learning_rate']\n",
    "max_iters = opt_parameters['max_iters']\n",
    "batch_iters = opt_parameters['batch_iters']\n",
    "decay_rate = opt_parameters['decay_rate']\n",
    "\n",
    "# Optimizer\n",
    "global_lr = learning_rate\n",
    "global_step = 0\n",
    "lr = learning_rate\n",
    "optimizer = net.update(lr) \n",
    "\n",
    "# Statistics\n",
    "t_start = time.time()\n",
    "t_start_total = time.time()\n",
    "average_loss_old = 1e10\n",
    "running_loss = 0.0\n",
    "running_total = 0\n",
    "    \n",
    "for iteration in range(0, max_iters):\n",
    "    \n",
    "    for G in all_train_data:\n",
    "        # Forward pass\n",
    "        y_pred = net.forward(G)\n",
    "\n",
    "        # Target embedding matrix\n",
    "        y_true = G.target\n",
    "        y_true = Variable(torch.FloatTensor(y_true).type(dtypeFloat) , requires_grad=False) \n",
    "\n",
    "        # Compute L2 loss\n",
    "        #loss = net.loss(y_pred, y_true)\n",
    "        loss = net.pairwise_loss(y_pred, y_true, G.adj_matrix)\n",
    "        loss_train = loss.data[0]\n",
    "        running_loss += loss_train\n",
    "        running_total += 1\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # learning rate, print results\n",
    "    if not iteration%batch_iters:\n",
    "\n",
    "        # time\n",
    "        t_stop = time.time() - t_start\n",
    "        t_start = time.time()\n",
    "\n",
    "        # update learning rate \n",
    "        average_loss = running_loss/ running_total\n",
    "        if average_loss > 0.99* average_loss_old:\n",
    "            lr /= decay_rate\n",
    "        average_loss_old = average_loss\n",
    "        optimizer = net.update_learning_rate(optimizer, lr)\n",
    "        running_loss = 0.0\n",
    "        running_total = 0\n",
    "\n",
    "        # print results\n",
    "        if 1==1:\n",
    "            print('\\niteration= %d, loss(%diter)= %.8f, lr= %.8f, time(%diter)= %.2f' % \n",
    "                  (iteration, batch_iters, average_loss, lr, batch_iters, t_stop))\n",
    "    \n",
    "    if opt_parameters['save_flag'] and not (iteration+1)%checkpoint_interval:\n",
    "        filename = checkpoint_root + task_parameters['conv_type'] + str(int((iteration+1)/checkpoint_interval)) + '.pkl'\n",
    "        save_checkpoint({\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs, labels = draw_random_subset(test_data, num_samples=500)\n",
    "G = DataEmbeddingGraph(inputs, labels, task_parameters['reduction_method'])\n",
    "net_time_start = timer()\n",
    "if torch.cuda.is_available():   \n",
    "    y_pred = net.forward(G).cpu().detach().numpy()\n",
    "else:    \n",
    "    y_pred = net.forward(G).detach().numpy()\n",
    "net_time_end = timer()\n",
    "\n",
    "f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(10, 3), dpi=150)\n",
    "reduction_title = task_parameters['reduction_method'] + \"\\n time elapsed = {:.2f}s\".format(G.time_to_compute) \n",
    "cnn_title = \"Graph CNN\" + \"\\n time elapsed = {:.2f}s\".format(net_time_end - net_time_start) \n",
    "plot_embedding_subplot(axarr[0], G.target, G.labels.numpy(), reduction_title)\n",
    "plot_embedding_subplot(axarr[1], y_pred, G.labels.numpy(), cnn_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "if opt_parameters['save_flag']:\n",
    "    metadata_filename = checkpoint_root + \"experiment_metadata.txt\"\n",
    "    json.dump([opt_parameters, net_parameters], open(metadata_filename,'w'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
