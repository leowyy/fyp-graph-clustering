{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda available\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import os\n",
    "import time\n",
    "from timeit import default_timer as timer\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import pathlib\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    print('cuda available')\n",
    "    dtypeFloat = torch.cuda.FloatTensor\n",
    "    dtypeLong = torch.cuda.LongTensor\n",
    "else:\n",
    "    print('cuda not available')\n",
    "    dtypeFloat = torch.FloatTensor\n",
    "    dtypeLong = torch.LongTensor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.SimpleConvNet import SimpleConvNet\n",
    "from core.GraphConvNet2 import GraphConvNet2\n",
    "from core.DataEmbeddingGraph import DataEmbeddingGraph\n",
    "from util.plot_embedding import plot_embedding_subplot\n",
    "from util.draw_random_subset import draw_random_subset\n",
    "from util.evaluation_metrics import evaluate_net_metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def save_checkpoint(state, filename):\n",
    "    torch.save(state, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = {'mnist': '/data/set_20000_mnist_tsne.pkl', \n",
    "             'usps': '/data/set_7291_usps_tsne.pkl', \n",
    "             '20news': '/data/20news_lda_train_tsne.pkl', \n",
    "             'yux': '/data/yux_train_tsne_shuffle.pkl'}\n",
    "\n",
    "test_dir = {'mnist': '/data/set_100_mnist_spectral_size_200_500.pkl', \n",
    "            'usps': '/data/set_100_usps_spectral_size_200_500.pkl', \n",
    "            '20news': '/data/20news_lda_test_tsne.pkl', \n",
    "            'yux': None}\n",
    "\n",
    "max_train_size = {'mnist': 20000,\n",
    "                  'usps': 7291, \n",
    "                  '20news': 4109, \n",
    "                  'yux': 7000}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "task_parameters = {}\n",
    "task_parameters['dataset'] = 'yux'\n",
    "task_parameters['net_type'] = 'graph_net'\n",
    "task_parameters['reduction_method'] = 'preprocessed_tsne'\n",
    "task_parameters['loss_function'] = 'composite_loss'\n",
    "task_parameters['n_components'] = 2\n",
    "task_parameters['train_dir'] = train_dir[task_parameters['dataset']]\n",
    "task_parameters['test_dir'] = test_dir[task_parameters['dataset']]\n",
    "task_parameters['val_flag'] = False\n",
    "\n",
    "net_parameters = {}\n",
    "net_parameters['n_components'] = task_parameters['n_components']\n",
    "net_parameters['D'] = 66 # input dimension\n",
    "net_parameters['H'] = 50 # number of hidden units\n",
    "net_parameters['L'] = 10 # number of hidden layers\n",
    "net_parameters['n_channels'] = 1 # for conv net\n",
    "net_parameters['n_units_1'] = net_parameters['n_units_2'] = net_parameters['H'] # for conv net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "if task_parameters['net_type'] == 'graph_net':\n",
    "    net = GraphConvNet2(net_parameters)\n",
    "elif task_parameters['net_type'] == 'conv_net':\n",
    "    net = SimpleConvNet(net_parameters)\n",
    "if torch.cuda.is_available():\n",
    "    net.cuda()\n",
    "# print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# optimization parameters\n",
    "opt_parameters = {}\n",
    "opt_parameters['learning_rate'] = 0.00075   # ADAM\n",
    "opt_parameters['max_iters'] = 200   \n",
    "opt_parameters['batch_iters'] = 10\n",
    "opt_parameters['max_train_size'] = max_train_size[task_parameters['dataset']]  \n",
    "opt_parameters['save_flag'] = True\n",
    "\n",
    "if 2==1: # fast debugging\n",
    "    opt_parameters['max_iters'] = 5 \n",
    "    opt_parameters['batch_iters'] = 1\n",
    "    opt_parameters['save_flag'] = False\n",
    "\n",
    "opt_parameters['decay_rate'] = 1.25   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "if opt_parameters['save_flag']: \n",
    "    checkpoint_interval = opt_parameters['max_iters']/5\n",
    "    checkpoint_root = 'results/' + task_parameters['dataset'] +'_'+ task_parameters['reduction_method'] + '2/'\n",
    "    pathlib.Path(checkpoint_root).mkdir(exist_ok=True) # create the directory if it doesn't exist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pre-processed train data\n",
    "parent_dir = os.path.abspath('..')\n",
    "with open(parent_dir+task_parameters['train_dir'], 'rb') as f:\n",
    "    [inputs, labels, X_emb] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(7000, 66)\n",
      "0\n",
      "(7000, 2)\n"
     ]
    }
   ],
   "source": [
    "print(inputs.shape)\n",
    "print(len(labels))\n",
    "print(X_emb.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Do not perform validation checks if the data is unlabelled\n",
    "if len(labels) == 0:\n",
    "    task_parameters['val_flag'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_train_data = []\n",
    "num_train_samples = 0\n",
    "while num_train_samples <= opt_parameters['max_train_size']:\n",
    "    # Draw a random training batch of variable size\n",
    "    num_samples = np.random.randint(200, 500)\n",
    "    inputs_subset = inputs[num_train_samples:num_train_samples+num_samples]\n",
    "    X_emb_subset = X_emb[num_train_samples:num_train_samples+num_samples]\n",
    "    \n",
    "    if task_parameters['val_flag']:\n",
    "        labels_subset = labels[num_train_samples:num_train_samples+num_samples]\n",
    "    else:\n",
    "        labels_subset = []\n",
    "    \n",
    "    # Package into graph block\n",
    "    G = DataEmbeddingGraph(inputs_subset, labels_subset, method=None)\n",
    "    G.target = X_emb_subset # replace target with pre-computed embeddings\n",
    "    \n",
    "    all_train_data.append(G)\n",
    "    num_train_samples += num_samples  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "if all_train_data[-1].data.shape[0] < 200:\n",
    "    print('Removing the last block...')\n",
    "    all_train_data = all_train_data[:-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset sizes = [250, 292, 255, 246, 326, 488, 283, 315, 320, 372, 348, 487, 497, 245, 376, 407, 411, 496, 338, 248]\n",
      "Total number of samples = 7000\n"
     ]
    }
   ],
   "source": [
    "dataset_sizes = [G.data.shape[0] for G in all_train_data]\n",
    "print(\"Dataset sizes = {}\".format(dataset_sizes))\n",
    "print(\"Total number of samples = {}\".format(sum(dataset_sizes)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test set\n",
    "if task_parameters['val_flag']:\n",
    "    with open(parent_dir+task_parameters['test_dir'], 'rb') as f:\n",
    "        [all_test_data] = pickle.load(f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/leowyaoyang/miniconda3/lib/python3.6/site-packages/ipykernel_launcher.py:39: UserWarning: invalid index of a 0-dim tensor. This will be an error in PyTorch 0.5. Use tensor.item() to convert a 0-dim tensor to a Python number\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "iteration= 0, loss(10iter)= 1226.55383301, lr= 0.00075000, time(10iter)= 4.86\n",
      "\n",
      "iteration= 10, loss(10iter)= 492.89666748, lr= 0.00075000, time(10iter)= 38.27\n",
      "\n",
      "iteration= 20, loss(10iter)= 112.72266388, lr= 0.00075000, time(10iter)= 38.10\n",
      "\n",
      "iteration= 30, loss(10iter)= 79.73033142, lr= 0.00075000, time(10iter)= 38.39\n",
      "\n",
      "iteration= 40, loss(10iter)= 64.98644257, lr= 0.00075000, time(10iter)= 39.02\n",
      "\n",
      "iteration= 50, loss(10iter)= 50.87390518, lr= 0.00075000, time(10iter)= 40.24\n",
      "\n",
      "iteration= 60, loss(10iter)= 44.69409561, lr= 0.00075000, time(10iter)= 40.90\n",
      "\n",
      "iteration= 70, loss(10iter)= 41.57003784, lr= 0.00075000, time(10iter)= 41.32\n",
      "\n",
      "iteration= 80, loss(10iter)= 34.00128937, lr= 0.00075000, time(10iter)= 40.92\n",
      "\n",
      "iteration= 90, loss(10iter)= 30.53816605, lr= 0.00075000, time(10iter)= 40.36\n",
      "\n",
      "iteration= 100, loss(10iter)= 29.27166176, lr= 0.00075000, time(10iter)= 40.32\n",
      "\n",
      "iteration= 110, loss(10iter)= 24.91920280, lr= 0.00075000, time(10iter)= 40.69\n",
      "\n",
      "iteration= 120, loss(10iter)= 26.67743301, lr= 0.00060000, time(10iter)= 40.61\n",
      "\n",
      "iteration= 130, loss(10iter)= 24.57504845, lr= 0.00060000, time(10iter)= 40.92\n",
      "\n",
      "iteration= 140, loss(10iter)= 18.05130768, lr= 0.00060000, time(10iter)= 40.84\n",
      "\n",
      "iteration= 150, loss(10iter)= 17.29617119, lr= 0.00060000, time(10iter)= 40.99\n",
      "\n",
      "iteration= 160, loss(10iter)= 17.28682899, lr= 0.00048000, time(10iter)= 41.02\n",
      "\n",
      "iteration= 170, loss(10iter)= 16.91546822, lr= 0.00048000, time(10iter)= 40.92\n",
      "\n",
      "iteration= 180, loss(10iter)= 15.05689907, lr= 0.00048000, time(10iter)= 40.92\n",
      "\n",
      "iteration= 190, loss(10iter)= 15.81491661, lr= 0.00038400, time(10iter)= 40.70\n"
     ]
    }
   ],
   "source": [
    "# Optimization parameters\n",
    "learning_rate = opt_parameters['learning_rate']\n",
    "max_iters = opt_parameters['max_iters']\n",
    "batch_iters = opt_parameters['batch_iters']\n",
    "decay_rate = opt_parameters['decay_rate']\n",
    "\n",
    "# Optimizer\n",
    "global_lr = learning_rate\n",
    "global_step = 0\n",
    "lr = learning_rate\n",
    "optimizer = net.update(lr) \n",
    "\n",
    "# Statistics\n",
    "t_start = time.time()\n",
    "t_start_total = time.time()\n",
    "average_loss_old = 1e10\n",
    "running_loss = 0.0\n",
    "running_total = 0\n",
    "tab_results = []\n",
    "    \n",
    "for iteration in range(max_iters):\n",
    "    net.train()\n",
    "    for G in all_train_data:\n",
    "        # Forward pass\n",
    "        y_pred = net.forward(G)\n",
    "\n",
    "        # Target embedding matrix\n",
    "        y_true = G.target\n",
    "        y_true = Variable(torch.FloatTensor(y_true).type(dtypeFloat) , requires_grad=False) \n",
    "\n",
    "        # Compute overall loss \n",
    "        if task_parameters['loss_function'] == 'pairwise_loss':\n",
    "            loss = net.pairwise_loss(y_pred, y_true, G.adj_matrix)\n",
    "        elif task_parameters['loss_function'] == 'composite_loss':\n",
    "            loss1 = net.loss(y_pred, y_true)\n",
    "            loss2 = net.pairwise_loss(y_pred, y_true, G.adj_matrix)\n",
    "            loss = 0.5*loss1 + 0.5*loss2\n",
    "            \n",
    "        loss_train = loss.data[0]\n",
    "        running_loss += loss_train\n",
    "        running_total += 1\n",
    "\n",
    "        # Backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "    # learning rate, print results\n",
    "    if not iteration%batch_iters:\n",
    "\n",
    "        # time\n",
    "        t_stop = time.time() - t_start\n",
    "        t_start = time.time()\n",
    "\n",
    "        # update learning rate \n",
    "        average_loss = running_loss/ running_total\n",
    "        if average_loss > 0.99* average_loss_old:\n",
    "            lr /= decay_rate\n",
    "        average_loss_old = average_loss\n",
    "        optimizer = net.update_learning_rate(optimizer, lr)\n",
    "        running_loss = 0.0\n",
    "        running_total = 0\n",
    "\n",
    "        # print results\n",
    "        print('\\niteration= %d, loss(%diter)= %.8f, lr= %.8f, time(%diter)= %.2f' %\n",
    "              (iteration, batch_iters, average_loss, lr, batch_iters, t_stop))\n",
    "            \n",
    "        # validate on test set\n",
    "        #######################\n",
    "        if task_parameters['val_flag']:\n",
    "            trustworthiness, one_nn, five_nn, time_elapsed = evaluate_net_metrics(all_test_data, net)\n",
    "            print(\"Trust = {:.4f}, 1-NN = {:.4f}, 5-NN = {:.4f}, time to compute = {:.2f}s\".format(trustworthiness, one_nn, five_nn, time_elapsed))\n",
    "            tab_results.append([iteration, average_loss, trustworthiness, one_nn, five_nn, time.time()-t_start_total])\n",
    "    \n",
    "    if opt_parameters['save_flag'] and not (iteration+1)%checkpoint_interval:\n",
    "        filename = checkpoint_root + task_parameters['net_type'] + str(int((iteration+1)/checkpoint_interval)) + '.pkl'\n",
    "        save_checkpoint({\n",
    "            'state_dict': net.state_dict(),\n",
    "            'optimizer' : optimizer.state_dict(),\n",
    "        }, filename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save metadata \n",
    "if opt_parameters['save_flag']:\n",
    "    metadata_filename = checkpoint_root + \"experiment_metadata.txt\"\n",
    "    with open(metadata_filename, 'w') as f:\n",
    "        f.write('-----------------------\\n')\n",
    "        f.write('\\n'.join([\"%s = %s\" % (k,v) for k,v in task_parameters.items()]))\n",
    "        \n",
    "        f.write('\\n-----------------------\\n')\n",
    "        f.write('\\n'.join([\"%s = %s\" % (k,v) for k,v in net_parameters.items()]))\n",
    "        \n",
    "        f.write('\\n-----------------------\\n')\n",
    "        f.write('\\n'.join([\"%s = %s\" % (k,v) for k,v in opt_parameters.items()]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save logs \n",
    "if opt_parameters['save_flag'] and task_parameters['val_flag']:\n",
    "    logs_filename = checkpoint_root + \"experiment_results.pkl\"\n",
    "    with open(logs_filename, 'wb') as f:\n",
    "        pickle.dump(tab_results, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AssertionError",
     "evalue": "Stopped further execution because val_flag is off",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-c165d544710c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# Continue execution only if val_flag is True\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0;32massert\u001b[0m \u001b[0mtask_parameters\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'val_flag'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'Stopped further execution because val_flag is off'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mAssertionError\u001b[0m: Stopped further execution because val_flag is off"
     ]
    }
   ],
   "source": [
    "# Continue execution only if val_flag is True\n",
    "assert task_parameters['val_flag'], 'Stopped further execution because val_flag is off'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot results\n",
    "iterations = [res[0] for res in tab_results]\n",
    "loss_score = [float(res[1]) for res in tab_results]\n",
    "trust_score = [res[2] for res in tab_results]\n",
    "one_nn_score = [res[3] for res in tab_results]\n",
    "five_nn_score = [res[4] for res in tab_results]\n",
    "max_loss = max(loss_score)\n",
    "loss_score = [res/max_loss for res in loss_score] # normalise losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 1, sharex='col', sharey='row', figsize=(7,5), dpi=150)\n",
    "plt.plot(iterations, loss_score, label='Training loss')\n",
    "plt.plot(iterations, trust_score, label='Trustworthiness')\n",
    "plt.plot(iterations, one_nn_score, label='1-NN accuracy')\n",
    "plt.plot(iterations, five_nn_score, label='5-NN accuracy')\n",
    "plt.grid()\n",
    "plt.legend()\n",
    "plt.xlabel('Iteration')\n",
    "if opt_parameters['save_flag']:\n",
    "    plot_filename = checkpoint_root + task_parameters['dataset'] +'_'+ task_parameters['reduction_method'] + \"_plot.png\"\n",
    "    plt.savefig(plot_filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## On test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs, labels = draw_random_subset(test_data, num_samples=500)\n",
    "# G = DataEmbeddingGraph(inputs, labels, task_parameters['reduction_method'])\n",
    "# net_time_start = timer()\n",
    "# if torch.cuda.is_available():   \n",
    "#     y_pred = net.forward(G).cpu().detach().numpy()\n",
    "# else:    \n",
    "#     y_pred = net.forward(G).detach().numpy()\n",
    "# net_time_end = timer()\n",
    "\n",
    "# f, axarr = plt.subplots(1, 2, sharex='col', sharey='row', figsize=(10, 3), dpi=150)\n",
    "# reduction_title = task_parameters['reduction_method'] + \"\\n time elapsed = {:.2f}s\".format(G.time_to_compute) \n",
    "# cnn_title = \"Graph CNN\" + \"\\n time elapsed = {:.2f}s\".format(net_time_end - net_time_start) \n",
    "# plot_embedding_subplot(axarr[0], G.target, G.labels.numpy(), reduction_title)\n",
    "# plot_embedding_subplot(axarr[1], y_pred, G.labels.numpy(), cnn_title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 20 colors\n",
    "colormap = np.array([\n",
    "    \"#1f77b4\", \"#aec7e8\", \"#ff7f0e\", \"#ffbb78\", \"#2ca02c\",\n",
    "    \"#98df8a\", \"#d62728\", \"#ff9896\", \"#9467bd\", \"#c5b0d5\",\n",
    "    \"#8c564b\", \"#c49c94\", \"#e377c2\", \"#f7b6d2\", \"#7f7f7f\",\n",
    "    \"#c7c7c7\", \"#bcbd22\", \"#dbdb8d\", \"#17becf\", \"#9edae5\"\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_embedding_subplot(ax, X, labels=None, title=None):\n",
    "    x_min, x_max = np.min(X, 0), np.max(X, 0)\n",
    "    X = (X - x_min) / (x_max - x_min)\n",
    "\n",
    "    if labels is not None:\n",
    "        for i in range(X.shape[0]):\n",
    "            ax.text(X[i, 0], X[i, 1], str(labels[i]),\n",
    "                    color=colormap[labels[i]],\n",
    "                    fontdict={'weight': 'bold', 'size': 9})\n",
    "\n",
    "    ax.set_xticks([]), ax.set_yticks([])\n",
    "    if title is not None:\n",
    "        ax.set_title(title)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "f, axarr = plt.subplots(1, 1, sharex='col', sharey='row', figsize=(10, 10))\n",
    "int_labels = [int(l) for l in labels]\n",
    "plot_embedding_subplot(axarr, X_emb, labels=int_labels, title='tsne')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from core.DimReduction import DimReduction\n",
    "\n",
    "f, axarr = plt.subplots(3, 4, sharex='col', sharey='row', figsize=(20, 10.5))\n",
    "dim_red = DimReduction(n_components=2)\n",
    "\n",
    "for i in range(4):\n",
    "    G = all_train_data[i]\n",
    "    int_labels = [int(l) for l in G.labels]\n",
    "    X = G.data.view(G.data.shape[0], -1).numpy()\n",
    "    \n",
    "    # tsne\n",
    "    X_emb1 = dim_red.fit_transform(X, 'tsne', labels)\n",
    "    plot_embedding_subplot(axarr[0,i], X_emb1, labels=int_labels, title='tsne')\n",
    "    \n",
    "    # ground truth\n",
    "    plot_embedding_subplot(axarr[1,i], G.target, labels=int_labels, title='ground_truth')\n",
    "    \n",
    "    # Net\n",
    "    if torch.cuda.is_available():   \n",
    "        y_pred = net.forward(G).cpu().detach().numpy()\n",
    "    else:    \n",
    "        y_pred = net.forward(G).detach().numpy()\n",
    "    plot_embedding_subplot(axarr[2,i], y_pred, labels=int_labels, title='tsne_net')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
